{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":200743,"sourceType":"datasetVersion","datasetId":87153}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Model and Library Imports\n\nIn this section, we import the necessary libraries and modules for building our deep learning model. We utilize Keras for constructing the neural network architecture, leveraging various pre-trained models such as VGG16, EfficientNetB3, ResNet50, and MobileNetV2. Additionally, we include utilities for image processing, data augmentation, and early stopping during training to prevent overfitting.\n\nThe following libraries are imported:\n- **TensorFlow/Keras**: For model building and training.\n- **NumPy**: For numerical operations.\n- **Matplotlib**: For visualizing results.\n- **Pandas**: For data manipulation.\n- **OpenCV**: For image processing.\n- **Glob**: For file handling.\n\nWe also suppress warnings to keep the output clean.\n","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.applications import VGG16, EfficientNetB3, ResNet50, MobileNetV2\nfrom tensorflow.keras.models import load_model\n\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport cv2\nimport os\nimport glob\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:57:14.691004Z","iopub.execute_input":"2024-09-19T09:57:14.691776Z","iopub.status.idle":"2024-09-19T09:57:14.700816Z","shell.execute_reply.started":"2024-09-19T09:57:14.691721Z","shell.execute_reply":"2024-09-19T09:57:14.699894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Exploration","metadata":{}},{"cell_type":"markdown","source":"### Dataset Paths\n\nIn this section, we define the paths to the dataset containing cell images used for detecting malaria. The dataset is organized into two main directories: one for parasitized cells and another for uninfected cells. We use the `os.path.join` method to create full paths for each category, ensuring compatibility across different operating systems.\n\n- **DATA_DIR**: The main directory where the cell images are stored.\n- **PARASITIZED_DIR**: Path to the directory containing images of parasitized cells.\n- **UNINFECTED_DIR**: Path to the directory containing images of uninfected cells.\n","metadata":{}},{"cell_type":"code","source":"# paths\nDATA_DIR = '/kaggle/input/cell-images-for-detecting-malaria/cell_images/cell_images'\n\nPARASITIZED_DIR = os.path.join(DATA_DIR, 'Parasitized')\nUNINFECTED_DIR = os.path.join(DATA_DIR, 'Uninfected')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:42:40.151054Z","iopub.execute_input":"2024-09-19T08:42:40.151580Z","iopub.status.idle":"2024-09-19T08:42:40.156099Z","shell.execute_reply.started":"2024-09-19T08:42:40.151545Z","shell.execute_reply":"2024-09-19T08:42:40.155158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset Size Overview\n\nIn this section, we print the number of images in each category of the dataset: parasitized and uninfected cells. This provides an initial overview of the dataset size, which is crucial for understanding the distribution of data and planning the model training process.\n\n- The number of **parasitized images** is displayed.\n- The number of **uninfected images** is displayed.\n","metadata":{}},{"cell_type":"code","source":"print(f'Parasitized images number = {len(os.listdir(PARASITIZED_DIR))}')\nprint(f'Uninfected images number = {len(os.listdir(UNINFECTED_DIR))}')","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:42:40.195192Z","iopub.execute_input":"2024-09-19T08:42:40.195904Z","iopub.status.idle":"2024-09-19T08:42:40.219371Z","shell.execute_reply.started":"2024-09-19T08:42:40.195868Z","shell.execute_reply":"2024-09-19T08:42:40.218496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset Size Output\n\nThe dataset contains an equal number of images in both categories, which is important for balanced training. \n\n- **Parasitized images number**: 13,780\n- **Uninfected images number**: 13,780\n\nThis balance helps ensure that the model does not become biased toward one class during training.\n","metadata":{}},{"cell_type":"markdown","source":"### Image Paths Collection\n\nIn this section, we gather the file paths for all images in the dataset. We use the `glob` library to retrieve the paths for both parasitized and uninfected images. The paths are stored in separate lists, which are then combined into a single list for further processing.\n\n- **parasitized_images_paths**: A list of file paths for all parasitized cell images.\n- **uninfected_images_paths**: A list of file paths for all uninfected cell images.\n- **all_images_paths**: A combined list containing the paths of both parasitized and uninfected images, facilitating streamlined data handling in subsequent steps.\n","metadata":{}},{"cell_type":"code","source":"parasitized_images_paths = glob.glob(os.path.join(PARASITIZED_DIR, '*'))\nuninfected_images_paths = glob.glob(os.path.join(UNINFECTED_DIR, '*'))\n\nall_images_paths = parasitized_images_paths + uninfected_images_paths","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:42:40.279069Z","iopub.execute_input":"2024-09-19T08:42:40.279487Z","iopub.status.idle":"2024-09-19T08:42:40.386777Z","shell.execute_reply.started":"2024-09-19T08:42:40.279451Z","shell.execute_reply":"2024-09-19T08:42:40.385719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Image Extensions Count\n\nIn this section, we analyze the file extensions of the images in the dataset. We create a dictionary to count the occurrences of each file extension among the collected image paths.\n\n- **extensions**: A dictionary where the keys are file extensions and the values are the counts of how many images have that extension.\n- The loop iterates through each image path, extracts the file extension, and updates the count in the dictionary accordingly.\n\nThis helps ensure that we are aware of the types of image files present in the dataset, which can be useful for preprocessing steps.\n","metadata":{}},{"cell_type":"code","source":"extensions = {}\n\nfor image_path in all_images_paths:\n    ext = image_path.split('.')[-1] # get the extension of a path such as image.png -> png\n    \n    if ext in extensions: \n        extensions[ext] += 1\n    else:\n        extensions[ext] = 1\nextensions","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:42:40.388803Z","iopub.execute_input":"2024-09-19T08:42:40.389231Z","iopub.status.idle":"2024-09-19T08:42:40.420420Z","shell.execute_reply.started":"2024-09-19T08:42:40.389183Z","shell.execute_reply":"2024-09-19T08:42:40.419483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **PNG files**: 27,558\n- **DB files**: 2\n\nThis indicates that the majority of images in the dataset are in PNG format, while there are very few database files present. Understanding the types of files helps inform the preprocessing and loading methods we will use for the model.\n","metadata":{}},{"cell_type":"markdown","source":"### Image Dimensions Extraction Function\n\nIn this section, we define a function to retrieve the dimensions of images stored in a specified directory. The function processes only PNG images and collects their widths and heights.\n\n- **Function**: `get_images_dimensions(images_dir)`\n  - **Input**: Directory path containing images.\n  - **Output**: A dictionary with two lists:\n    - **widths**: List of widths for all processed images.\n    - **heights**: List of heights for all processed images.\n\nThe function uses OpenCV to read each image and extract its dimensions, which are important for understanding the input size for the model and ensuring consistency in preprocessing.\n","metadata":{}},{"cell_type":"code","source":"def get_images_dimensions(images_dir):\n    \n    images_paths = glob.glob(os.path.join(images_dir, '*'))\n\n    dimensions = {'widths': [],\n                   'heights': []}\n    \n    for image_path in images_paths:\n        if image_path.split('.')[-1] == 'png':\n            image = cv2.imread(image_path)\n            w, h, _ = image.shape \n            dimensions['widths'].append(w)\n            dimensions['heights'].append(h)\n            \n    return dimensions","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:42:40.421619Z","iopub.execute_input":"2024-09-19T08:42:40.421946Z","iopub.status.idle":"2024-09-19T08:42:40.428181Z","shell.execute_reply.started":"2024-09-19T08:42:40.421915Z","shell.execute_reply":"2024-09-19T08:42:40.427099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parasitized_dimensions = get_images_dimensions(PARASITIZED_DIR)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:42:40.430130Z","iopub.execute_input":"2024-09-19T08:42:40.430463Z","iopub.status.idle":"2024-09-19T08:42:59.945694Z","shell.execute_reply.started":"2024-09-19T08:42:40.430430Z","shell.execute_reply":"2024-09-19T08:42:59.944596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"uninfected_dimensions = get_images_dimensions(UNINFECTED_DIR)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:42:59.946975Z","iopub.execute_input":"2024-09-19T08:42:59.947406Z","iopub.status.idle":"2024-09-19T08:43:18.573077Z","shell.execute_reply.started":"2024-09-19T08:42:59.947361Z","shell.execute_reply":"2024-09-19T08:43:18.571989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualization of Image Dimensions\n\nIn this section, we create scatter plots to visualize the widths and heights of parasitized and uninfected images. \n\n- The left plot displays the dimensions of **parasitized images**.\n- The right plot shows the dimensions of **uninfected images**.\n\nThis visualization helps us understand the distribution of image sizes in the dataset, which is essential for ensuring consistency during preprocessing and model training. Variability in dimensions can affect how data generators handle images.\n","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(10, 6))\n\naxes[0].scatter(parasitized_dimensions['widths'], parasitized_dimensions['heights'], alpha=0.4, label='parasitized')\naxes[0].set_xlabel('images widths')\naxes[0].set_ylabel('images heights')\naxes[0].legend()\n\naxes[1].scatter(uninfected_dimensions['widths'], uninfected_dimensions['heights'], color='red',alpha=0.4, label='uninfected')\naxes[1].set_xlabel('images widths')\naxes[1].set_ylabel('images heights')\naxes[1].legend()\n\nfig.suptitle('images widths VS heights')\nplt.tight_layout()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:43:18.574225Z","iopub.execute_input":"2024-09-19T08:43:18.574545Z","iopub.status.idle":"2024-09-19T08:43:19.995804Z","shell.execute_reply.started":"2024-09-19T08:43:18.574512Z","shell.execute_reply":"2024-09-19T08:43:19.994812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Input Shape for Model\n\nFor model training, we standardize all images to a shape of **(128, 128, 3)**. \n\n","metadata":{}},{"cell_type":"markdown","source":"### Sample Images Visualization\n\nIn this section, we display sample images from both the parasitized and uninfected cell datasets.\n\n- The left plot shows a **parasitized cell**.\n- The right plot shows an **uninfected cell**.\n\nThese visualizations help provide a clearer understanding of the dataset, allowing for a qualitative assessment of the image quality and characteristics before model training.\n","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(10, 6))\n\nimage = cv2.imread(parasitized_images_paths[0])\nimage_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\naxes[0].set_title('parasitized cell')\naxes[0].imshow(image)\naxes[0].axis('off')\n\n\nimage = cv2.imread(uninfected_images_paths[0])\nimage_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\naxes[1].set_title('uninfected cell')\naxes[1].imshow(image)\naxes[1].axis('off')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:43:19.999042Z","iopub.execute_input":"2024-09-19T08:43:19.999453Z","iopub.status.idle":"2024-09-19T08:43:20.412233Z","shell.execute_reply.started":"2024-09-19T08:43:19.999408Z","shell.execute_reply":"2024-09-19T08:43:20.411187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pixel Value Analysis and Normalization Check\n\nIn this section, we analyze the pixel values of sample images from both the parasitized and uninfected datasets by checking their maximum and minimum values:\n\n- The **maximum pixel value** represents the brightest pixel in the image.\n- The **minimum pixel value** represents the darkest pixel.\n\nBy examining these values, we can determine if the data needs normalization. If the pixel values range from 0 to 255, normalization (e.g., scaling between 0 and 1) may be required to optimize model performance.\n","metadata":{}},{"cell_type":"code","source":"print(f' max pixel value of parasitized image {cv2.imread(parasitized_images_paths[0]).max()}')\nprint(f' min pixel value of parasitized image {cv2.imread(parasitized_images_paths[0]).min()}')\nprint(f' max pixel value of uninfected image {cv2.imread(uninfected_images_paths[0]).max()}')\nprint(f' min pixel value of uninfected image {cv2.imread(uninfected_images_paths[0]).min()}')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:43:20.413679Z","iopub.execute_input":"2024-09-19T08:43:20.414143Z","iopub.status.idle":"2024-09-19T08:43:20.428037Z","shell.execute_reply.started":"2024-09-19T08:43:20.414093Z","shell.execute_reply":"2024-09-19T08:43:20.427157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the pixel values do not span the full [0, 255] range, normalization (scaling between 0 and 1) is necessary to ensure consistent model input and improve training efficiency.\n","metadata":{}},{"cell_type":"markdown","source":"# Data Rescaling and Validation Split\n\nIn this section, we use Keras's `ImageDataGenerator` for rescaling the pixel values and splitting the dataset for training and validation.\n\n- **Rescaling**: The pixel values are normalized between 0 and 1 using `rescale=1/255.0`.\n- **Validation Split**: 20% of the data is reserved for validation using `validation_split=0.2`.\n\nThis step ensures that the model receives normalized inputs without performing data augmentation.\n","metadata":{}},{"cell_type":"code","source":"data_generator = ImageDataGenerator(rescale=1/255.0, validation_split = 0.2)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:43:20.429309Z","iopub.execute_input":"2024-09-19T08:43:20.430026Z","iopub.status.idle":"2024-09-19T08:43:20.435122Z","shell.execute_reply.started":"2024-09-19T08:43:20.429976Z","shell.execute_reply":"2024-09-19T08:43:20.433922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create train data\ntrain_data = data_generator.flow_from_directory(\n    directory=DATA_DIR,\n    target_size=(128, 128),\n    class_mode='binary',\n    subset='training'\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:43:20.436483Z","iopub.execute_input":"2024-09-19T08:43:20.437519Z","iopub.status.idle":"2024-09-19T08:43:21.383230Z","shell.execute_reply.started":"2024-09-19T08:43:20.437462Z","shell.execute_reply":"2024-09-19T08:43:21.382381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note**:  \nIf you encounter the error `Found 22048 images belonging to 3 classes` instead of 2, ensure that your `DATA_DIR` is set to `/kaggle/input/cell-images-for-detecting-malaria/cell_images/cell_images` rather than `/kaggle/input/cell-images-for-detecting-malaria/cell_images`. When unzipping a directory, it may create an additional directory with the same name, so make sure the file path is correct.","metadata":{}},{"cell_type":"code","source":"test_data = data_generator.flow_from_directory(\n    directory=DATA_DIR,\n    target_size=(128, 128),\n    class_mode='binary',\n    subset='validation'\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:43:21.384321Z","iopub.execute_input":"2024-09-19T08:43:21.384602Z","iopub.status.idle":"2024-09-19T08:43:21.714369Z","shell.execute_reply.started":"2024-09-19T08:43:21.384572Z","shell.execute_reply":"2024-09-19T08:43:21.713606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Convolutional Neural Network (CNN) Model\n\nIn this section, we define a simple CNN architecture for binary classification of parasitized and uninfected cells.\n\n- **Conv2D Layers**: \n  - The model starts with two convolutional layers with 32 and 64 filters, each using a `3x3` kernel and ReLU activation.\n  - Both layers apply **same padding** to retain image dimensions.\n  \n- **MaxPooling2D**: After each convolution, max-pooling with a `2x2` window is applied to reduce spatial dimensions.\n\n- **Flatten Layer**: Converts the 2D output into a 1D vector.\n\n- **Dense Layers**: \n  - Two fully connected layers with 128 units and ReLU activation.\n  - A **Dropout** layer with a rate of 0.5 to prevent overfitting.\n  - A final dense layer with 64 units.\n\n- **Output Layer**: A single output neuron with a sigmoid activation for binary classification (parasitized or uninfected).\n\nThis model processes the normalized `(128, 128, 3)` image inputs and outputs a probability for each class.\n","metadata":{}},{"cell_type":"code","source":"cnn_model = Sequential([\n    Conv2D(filters=32, kernel_size=3, padding='same', activation='relu',\n          input_shape=[128, 128, 3]),\n    MaxPooling2D(pool_size=(2,2)),\n    Conv2D(filters=64, kernel_size=3, padding='same', activation='relu',\n          input_shape=[128, 128, 3]),\n    MaxPooling2D(pool_size=(2,2)),\n    \n    Flatten(),\n    Dense(128, activation='relu'),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(64, activation='relu'),\n    Dense(1, activation='sigmoid')\n])","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:43:21.715478Z","iopub.execute_input":"2024-09-19T08:43:21.715787Z","iopub.status.idle":"2024-09-19T08:43:21.779993Z","shell.execute_reply.started":"2024-09-19T08:43:21.715735Z","shell.execute_reply":"2024-09-19T08:43:21.779158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:43:21.781182Z","iopub.execute_input":"2024-09-19T08:43:21.781487Z","iopub.status.idle":"2024-09-19T08:43:21.808617Z","shell.execute_reply.started":"2024-09-19T08:43:21.781453Z","shell.execute_reply":"2024-09-19T08:43:21.807642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_model.compile(optimizer='adam',\n             loss='binary_crossentropy',\n             metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:43:21.809878Z","iopub.execute_input":"2024-09-19T08:43:21.810198Z","iopub.status.idle":"2024-09-19T08:43:21.818851Z","shell.execute_reply.started":"2024-09-19T08:43:21.810163Z","shell.execute_reply":"2024-09-19T08:43:21.817788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:43:21.823180Z","iopub.execute_input":"2024-09-19T08:43:21.823482Z","iopub.status.idle":"2024-09-19T08:43:21.827935Z","shell.execute_reply.started":"2024-09-19T08:43:21.823450Z","shell.execute_reply":"2024-09-19T08:43:21.826936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_history = cnn_model.fit(train_data, epochs=20, callbacks=[early_stopping], validation_data=test_data)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:43:21.829092Z","iopub.execute_input":"2024-09-19T08:43:21.829360Z","iopub.status.idle":"2024-09-19T08:50:35.329089Z","shell.execute_reply.started":"2024-09-19T08:43:21.829322Z","shell.execute_reply":"2024-09-19T08:50:35.328161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_model.evaluate(train_data)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:50:35.330325Z","iopub.execute_input":"2024-09-19T08:50:35.330642Z","iopub.status.idle":"2024-09-19T08:51:11.992035Z","shell.execute_reply.started":"2024-09-19T08:50:35.330609Z","shell.execute_reply":"2024-09-19T08:51:11.991002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_model.evaluate(test_data)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:51:11.993362Z","iopub.execute_input":"2024-09-19T08:51:11.993664Z","iopub.status.idle":"2024-09-19T08:51:22.091867Z","shell.execute_reply.started":"2024-09-19T08:51:11.993631Z","shell.execute_reply":"2024-09-19T08:51:22.090962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Training History Plot\n\nIn this section, we define the `plot_history` function to visualize the training and validation accuracy and loss over epochs.\n\n\nThese plots are essential for identifying any signs of overfitting or underfitting during the training process.\n","metadata":{}},{"cell_type":"code","source":"def plot_history(history):\n    fig, axes = plt.subplots(ncols=2, figsize=(10, 4))\n    axes[0].plot(history.history['accuracy'], label='train acc')\n    axes[0].plot(history.history['val_accuracy'], label='test acc')\n\n    axes[0].set_xlabel('number of epochs')\n    axes[0].set_ylabel('accuracy')\n    axes[0].set_title('train vs test accuracy')\n    plt.legend()\n    \n    axes[1].plot(history.history['loss'], label='train loss')\n    axes[1].plot(history.history['val_loss'], label='test loss')\n\n    axes[1].set_xlabel('number of epochs')\n    axes[1].set_ylabel('loss')\n    axes[1].set_title('train vs test loss')\n    plt.legend()\n    \n    plt.show()\n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:51:22.093242Z","iopub.execute_input":"2024-09-19T08:51:22.093625Z","iopub.status.idle":"2024-09-19T08:51:22.101623Z","shell.execute_reply.started":"2024-09-19T08:51:22.093581Z","shell.execute_reply":"2024-09-19T08:51:22.100733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(cnn_history)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:51:22.102897Z","iopub.execute_input":"2024-09-19T08:51:22.103247Z","iopub.status.idle":"2024-09-19T08:51:22.514914Z","shell.execute_reply.started":"2024-09-19T08:51:22.103204Z","shell.execute_reply":"2024-09-19T08:51:22.513944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine-Tuning a Pretrained Model\n","metadata":{}},{"cell_type":"markdown","source":"In this section, we define the `finetune_basemodel` function to fine-tune a base model (e.g., VGG16, ResNet) pretrained on the ImageNet dataset for our specific task of binary classification.\n\n- **Base Model**: \n  - The function takes a `base_model` as input (such as VGG16 or ResNet50) and loads pretrained weights from ImageNet.\n  - The top layers of the base model are excluded (`include_top=False`), and its layers are frozen (non-trainable).\n\n- **Custom Layers**: \n  - After the base model, a `Flatten` layer is added, followed by a dense layer with 128 units and ReLU activation.\n  - A **Dropout** layer is used to reduce overfitting, followed by a single output neuron with a sigmoid activation for binary classification.\n\n- **Compilation and Training**: \n  - The model is compiled using the Adam optimizer and binary cross-entropy loss, suitable for binary classification tasks.\n  - **Early Stopping** is applied to prevent overfitting by monitoring validation loss with a patience of 5 epochs.\n\n- **Performance Evaluation**: After training, the model evaluates both training and test datasets to print the accuracy results. The function also calls `plot_history` to visualize the accuracy and loss trends.\n\n","metadata":{}},{"cell_type":"code","source":"def finetune_basemodel(base_model, input_shape=(128, 128, 3)):\n    base_model = base_model(weights='imagenet', include_top=False, input_shape=input_shape)\n\n    for layer in base_model.layers:\n        layer.trainable = False\n    model = Sequential()\n    model.add(base_vgg_model)\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    \n    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n    \n    history = model.fit(train_data, epochs=20, callbacks=[early_stopping], validation_data=test_data)\n    \n    print(f'Train Accuracy = {model.evaluate(train_data)}')\n    print(f'Test Accuracy = {model.evaluate(test_data)}')\n    \n    plot_history(history)\n    \n    return model, history","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:51:22.841220Z","iopub.execute_input":"2024-09-19T08:51:22.841556Z","iopub.status.idle":"2024-09-19T08:51:22.849620Z","shell.execute_reply.started":"2024-09-19T08:51:22.841521Z","shell.execute_reply":"2024-09-19T08:51:22.848605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fine-Tuning VGG16 Model\n- **VGG16 Overview**: VGG16 is characterized by its deep architecture, consisting of 16 layers with learnable weights. It primarily uses small convolutional filters (3x3) and is known for its simplicity and effectiveness in extracting features from images.\n\n- **Output**: The trained VGG16 model and its training history are stored in the variables `vgg_model` and `vgg_history`, respectively.\n\nThis approach harnesses the power of VGG16 to enhance the performance of our classification task.\n","metadata":{}},{"cell_type":"code","source":"vgg_model, vgg_history = finetune_basemodel(VGG16)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T08:51:22.850799Z","iopub.execute_input":"2024-09-19T08:51:22.851114Z","iopub.status.idle":"2024-09-19T09:06:24.662205Z","shell.execute_reply.started":"2024-09-19T08:51:22.851082Z","shell.execute_reply":"2024-09-19T09:06:24.661228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fine-Tuning EfficientNetB3 Model\n\n- **EfficientNetB3 Overview**: EfficientNetB3 is part of the EfficientNet family, designed to balance model depth, width, and resolution. It uses a combination of depthwise separable convolutions and squeeze-and-excitation blocks, making it highly efficient for various image classification tasks.\n","metadata":{}},{"cell_type":"code","source":"efficientnetb3_model, efficientnetb3_history = finetune_basemodel(EfficientNetB3)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:06:24.663538Z","iopub.execute_input":"2024-09-19T09:06:24.663893Z","iopub.status.idle":"2024-09-19T09:20:12.763370Z","shell.execute_reply.started":"2024-09-19T09:06:24.663859Z","shell.execute_reply":"2024-09-19T09:20:12.762508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fine-Tuning ResNet50 Model\n\n- **ResNet50 Overview**: ResNet50 consists of 50 layers and introduces skip connections, which help mitigate the vanishing gradient problem in deep networks. This architecture allows for training very deep networks while maintaining high performance, making it particularly effective for various image classification tasks.\n","metadata":{}},{"cell_type":"code","source":"resnet50_model, resnet50_history = finetune_basemodel(ResNet50)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:20:12.769017Z","iopub.execute_input":"2024-09-19T09:20:12.769338Z","iopub.status.idle":"2024-09-19T09:30:22.469748Z","shell.execute_reply.started":"2024-09-19T09:20:12.769303Z","shell.execute_reply":"2024-09-19T09:30:22.468805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fine-Tuning MobileNetV2 Model\n\nIn this section, we fine-tune the **MobileNetV2** architecture, designed for efficient performance on mobile and edge devices.\n\n- **MobileNetV2 Overview**: MobileNetV2 uses an inverted residual structure and depthwise separable convolutions, optimizing both speed and accuracy. Its lightweight design makes it suitable for real-time applications and environments with limited computational resources, making it a popular choice for mobile image classification tasks.\n","metadata":{}},{"cell_type":"code","source":"mobilenetv2_model, mobilenetv2_history = finetune_basemodel(MobileNetV2)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:30:22.476777Z","iopub.execute_input":"2024-09-19T09:30:22.477065Z","iopub.status.idle":"2024-09-19T09:39:10.724952Z","shell.execute_reply.started":"2024-09-19T09:30:22.477034Z","shell.execute_reply":"2024-09-19T09:39:10.723995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Accuracies Comparison\n","metadata":{}},{"cell_type":"markdown","source":"\nIn this section, we evaluate the performance of the different models on both the training and test datasets and compile the results into a DataFrame for comparison.\n\n- **Models Evaluated**: The models included are:\n  - Basic CNN\n  - VGG16\n  - EfficientNetB3\n  - ResNet50\n  - MobileNetV2\n\n- **Accuracy Calculation**: The test and train accuracies are obtained by evaluating each model on their respective datasets. These results are stored in a pandas DataFrame for easy comparison.\n\n- **Sorting**: The DataFrame is sorted by test accuracy to facilitate the comparison of model performance.\n\nThis analysis provides a clear overview of which model performs best in terms of accuracy on the test dataset.\n","metadata":{}},{"cell_type":"code","source":"models_accuracies_df = pd.DataFrame({\n    'Model Name': ['basic_cnn', 'vgg16', 'efficientnetb3', 'resnet50', 'mobilenetv2'],\n    'Test Accuracy': [cnn_model.evaluate(test_data)[1], \n                     vgg_model.evaluate(test_data)[1],\n                     efficientnetb3_model.evaluate(test_data)[1],\n                     resnet50_model.evaluate(test_data)[1],\n                     mobilenetv2_model.evaluate(test_data)[1]\n                    ],\n    'Train Accuracy': [cnn_model.evaluate(train_data)[1], \n                     vgg_model.evaluate(train_data)[1],\n                     efficientnetb3_model.evaluate(train_data)[1],\n                     resnet50_model.evaluate(train_data)[1],\n                     mobilenetv2_model.evaluate(train_data)[1]\n                    ],\n})\n\nmodels_accuracies_df","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:39:10.726269Z","iopub.execute_input":"2024-09-19T09:39:10.726606Z","iopub.status.idle":"2024-09-19T09:43:07.789108Z","shell.execute_reply.started":"2024-09-19T09:39:10.726573Z","shell.execute_reply":"2024-09-19T09:43:07.788171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models_accuracies_df = models_accuracies_df.sort_values(by='Test Accuracy', ascending=False)\nmodels_accuracies_df","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:54:22.097064Z","iopub.execute_input":"2024-09-19T09:54:22.097475Z","iopub.status.idle":"2024-09-19T09:54:22.109411Z","shell.execute_reply.started":"2024-09-19T09:54:22.097440Z","shell.execute_reply":"2024-09-19T09:54:22.108297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Selection","metadata":{}},{"cell_type":"markdown","source":"MobileNetV2 and VGG16 have almost the same accuracy, so choose any of them. I will choose for now the VGG16 model.","metadata":{}},{"cell_type":"code","source":"best_model = vgg_model","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:55:28.416808Z","iopub.execute_input":"2024-09-19T09:55:28.417203Z","iopub.status.idle":"2024-09-19T09:55:28.421445Z","shell.execute_reply.started":"2024-09-19T09:55:28.417166Z","shell.execute_reply":"2024-09-19T09:55:28.420497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model.save('malaria-classification-model-using-vgg16.h5')","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:55:52.493679Z","iopub.execute_input":"2024-09-19T09:55:52.494120Z","iopub.status.idle":"2024-09-19T09:55:52.625864Z","shell.execute_reply.started":"2024-09-19T09:55:52.494065Z","shell.execute_reply":"2024-09-19T09:55:52.624830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Prediction","metadata":{}},{"cell_type":"markdown","source":"### Model prediction of a single image","metadata":{}},{"cell_type":"code","source":"model = load_model('/kaggle/working/malaria-classification-model-using-vgg16.h5')","metadata":{"execution":{"iopub.status.busy":"2024-09-19T09:57:24.681325Z","iopub.execute_input":"2024-09-19T09:57:24.681998Z","iopub.status.idle":"2024-09-19T09:57:24.881236Z","shell.execute_reply.started":"2024-09-19T09:57:24.681957Z","shell.execute_reply":"2024-09-19T09:57:24.880327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.class_indices","metadata":{"execution":{"iopub.status.busy":"2024-09-19T10:10:47.363890Z","iopub.execute_input":"2024-09-19T10:10:47.364620Z","iopub.status.idle":"2024-09-19T10:10:47.370891Z","shell.execute_reply.started":"2024-09-19T10:10:47.364581Z","shell.execute_reply":"2024-09-19T10:10:47.369929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_name = ['Parasitized', 'Uninfected']","metadata":{"execution":{"iopub.status.busy":"2024-09-19T10:15:29.544289Z","iopub.execute_input":"2024-09-19T10:15:29.544687Z","iopub.status.idle":"2024-09-19T10:15:29.549471Z","shell.execute_reply.started":"2024-09-19T10:15:29.544649Z","shell.execute_reply":"2024-09-19T10:15:29.548294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_prediction(image_path, true_label, input_size=(128, 128)):\n    image = cv2.imread(image_path) \n    \n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    image = cv2.resize(image, input_size)\n    image = np.expand_dims(image, axis=0)\n    image = image / 255.0\n    predicted_label = int(np.round(model.predict(image))[0][0])\n\n    \n    plt.imshow(image_rgb)\n    plt.title(f'predicted is {label_name[predicted_label]}, true is {true_label}')\n    plt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2024-09-19T10:15:51.041740Z","iopub.execute_input":"2024-09-19T10:15:51.042147Z","iopub.status.idle":"2024-09-19T10:15:51.048627Z","shell.execute_reply.started":"2024-09-19T10:15:51.042111Z","shell.execute_reply":"2024-09-19T10:15:51.047609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_prediction(parasitized_images_paths[5], 'Parasitized')","metadata":{"execution":{"iopub.status.busy":"2024-09-19T10:15:51.251593Z","iopub.execute_input":"2024-09-19T10:15:51.252486Z","iopub.status.idle":"2024-09-19T10:15:51.553616Z","shell.execute_reply.started":"2024-09-19T10:15:51.252438Z","shell.execute_reply":"2024-09-19T10:15:51.552685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Future Work\nFor future improvements, we could explore the following:\n- Implementing data augmentation techniques to improve model generalization.\n- Fine-tuning hyperparameters for better performance.\n- Experimenting with other advanced architectures such as DenseNet or transformers.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}